{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f263241f9f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./src')  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pygame\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from Agents.alpha_zero.model import SimpleAlphaZeroNet\n",
    "from Agents.alpha_zero.mcts_alpha_zero import MCTSAgent\n",
    "from Env.env import OthelloEnv\n",
    "from Agents.alpha_zero.train import train_loop\n",
    "\n",
    "\n",
    "\n",
    "# Définir une graine aléatoire pour la reproductibilité\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment, network, and MCTS agent.\n",
    "env = OthelloEnv()\n",
    "network = SimpleAlphaZeroNet()\n",
    "agent = MCTSAgent(network=network, num_simulations=50)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/euler03/projects/rl_gym/src/Agents/alpha_zero/self_play.py\n"
     ]
    }
   ],
   "source": [
    "import Agents.alpha_zero.self_play as sp\n",
    "print(sp.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Loss: 5.1582, Final Reward: 164.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_388245/527551018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Agents/alpha_zero/train.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_play_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Agents/alpha_zero/self_play.py\u001b[0m in \u001b[0;36mself_play_game\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"board\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstate_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action_and_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpolicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Agents/alpha_zero/mcts_alpha_zero.py\u001b[0m in \u001b[0;36mchoose_action_and_policy\u001b[0;34m(self, env, temperature)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_expanded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Agents/alpha_zero/mcts_alpha_zero.py\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, network)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mnew_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mchild_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTSNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Env/env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Mettre à jour les mouvements valides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_valid_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Si le joueur actuel n'a pas de mouvements valides, passer son tour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Env/env.py\u001b[0m in \u001b[0;36m_get_valid_moves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBOARD_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBOARD_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0mvalid_moves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Env/env.py\u001b[0m in \u001b[0;36m_is_valid_move\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Pour chaque direction, vérifier si on peut capturer des pièces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_capture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/rl_gym/src/Env/env.py\u001b[0m in \u001b[0;36m_can_capture\u001b[0;34m(self, row, col, dr, dc)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Vérifier s'il y a au moins une pièce adverse adjacente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mBOARD_SIZE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mBOARD_SIZE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_losses, training_rewards = train_loop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the trained model weights to a file\n",
    "torch.save(network.state_dict(), 'alpha_zero_model1.pt')\n",
    "print(\"Model saved to alpha_zero_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"Random\"\n",
    "    \n",
    "    def choose_action(self, env):\n",
    "        \"\"\"Choisit aléatoirement une action parmi les coups valides.\"\"\"\n",
    "        # Récupérer les coups valides\n",
    "        obs = env._get_observation()\n",
    "        valid_moves = [i for i, is_valid in enumerate(obs[\"valid_moves\"]) if is_valid == 1]\n",
    "        \n",
    "        # Si aucun coup valide, retourner une action par défaut\n",
    "        if not valid_moves:\n",
    "            return 0\n",
    "        \n",
    "        # Retourner un coup aléatoire\n",
    "        return random.choice(valid_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, opponent, num_games=100, render=False):\n",
    "    \"\"\"Évalue un agent contre un adversaire sur plusieurs parties.\"\"\"\n",
    "    env = OthelloEnv()\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for game in tqdm(range(num_games), desc=f\"{agent.name} vs {opponent.name}\"):\n",
    "        # Réinitialiser l'environnement\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        game_reward = 0\n",
    "        \n",
    "        # Jouer la partie\n",
    "        while not done:\n",
    "            # Déterminer quel agent joue (BLACK commence)\n",
    "            current_player = obs[\"current_player\"]\n",
    "            current_agent = agent if current_player == 0 else opponent  # 0 pour BLACK, 1 pour WHITE\n",
    "            \n",
    "            # Choisir une action\n",
    "            action = current_agent.choose_action(env)\n",
    "            \n",
    "            # Exécuter l'action\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Accumuler la récompense (du point de vue de l'agent évalué)\n",
    "            if current_player == 0:  # Si c'est notre agent qui a joué\n",
    "                game_reward += reward\n",
    "        \n",
    "        # Analyser le résultat\n",
    "        black_count, white_count = env._get_score()\n",
    "        if black_count > white_count:  # BLACK a gagné\n",
    "            wins += 1\n",
    "        elif white_count > black_count:  # WHITE a gagné\n",
    "            losses += 1\n",
    "        else:  # Match nul\n",
    "            draws += 1\n",
    "        \n",
    "        total_rewards += game_reward\n",
    "    \n",
    "    # Calculer les statistiques\n",
    "    win_rate = wins / num_games\n",
    "    avg_reward = total_rewards / num_games\n",
    "    \n",
    "    return {\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"draws\": draws,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"avg_reward\": avg_reward\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_game(agent1, agent2, delay=0.5):\n",
    "    \"\"\"Visualise une partie entre deux agents.\"\"\"\n",
    "    # Configuration de pygame\n",
    "    pygame.init()\n",
    "    \n",
    "    # Constantes pour l'interface graphique\n",
    "    SQUARE_SIZE = 60\n",
    "    BOARD_WIDTH = BOARD_SIZE * SQUARE_SIZE\n",
    "    INFO_PANEL_WIDTH = 300\n",
    "    WINDOW_WIDTH = BOARD_WIDTH + INFO_PANEL_WIDTH\n",
    "    WINDOW_HEIGHT = BOARD_WIDTH\n",
    "    BACKGROUND_COLOR = (0, 120, 0)  # Vert foncé\n",
    "    LINE_COLOR = (0, 0, 0)  # Noir\n",
    "    BLACK_COLOR = (0, 0, 0)  # Noir\n",
    "    WHITE_COLOR = (255, 255, 255)  # Blanc\n",
    "    INFO_PANEL_COLOR = (50, 50, 50)  # Gris foncé\n",
    "    TEXT_COLOR = (255, 255, 255)  # Blanc\n",
    "    VALID_MOVE_COLOR = (0, 255, 0, 150)  # Vert semi-transparent\n",
    "    \n",
    "    # Création de la fenêtre\n",
    "    screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
    "    pygame.display.set_caption(f\"{agent1.name} vs {agent2.name}\")\n",
    "    \n",
    "    # Initialisation des polices\n",
    "    title_font = pygame.font.SysFont(\"Arial\", 30, bold=True)\n",
    "    info_font = pygame.font.SysFont(\"Arial\", 20)\n",
    "    score_font = pygame.font.SysFont(\"Arial\", 24, bold=True)\n",
    "    \n",
    "    # Créer une surface pour les coups valides\n",
    "    valid_move_surface = pygame.Surface((SQUARE_SIZE, SQUARE_SIZE), pygame.SRCALPHA)\n",
    "    valid_move_surface.fill((0, 255, 0, 100))  # Vert semi-transparent\n",
    "    \n",
    "    # Horloge pour contrôler la vitesse\n",
    "    clock = pygame.time.Clock()\n",
    "    \n",
    "    # Initialiser l'environnement\n",
    "    env = OthelloEnv()\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    black_reward = 0\n",
    "    white_reward = 0\n",
    "    actions_history = []\n",
    "    \n",
    "    def draw_board():\n",
    "        \"\"\"Dessine le plateau de jeu.\"\"\"\n",
    "        # Fond du plateau\n",
    "        screen.fill(BACKGROUND_COLOR, (0, 0, BOARD_WIDTH, WINDOW_HEIGHT))\n",
    "        \n",
    "        # Lignes du plateau\n",
    "        for i in range(BOARD_SIZE + 1):\n",
    "            pygame.draw.line(screen, LINE_COLOR, (i * SQUARE_SIZE, 0), \n",
    "                             (i * SQUARE_SIZE, BOARD_WIDTH), 2)\n",
    "            pygame.draw.line(screen, LINE_COLOR, (0, i * SQUARE_SIZE), \n",
    "                             (BOARD_WIDTH, i * SQUARE_SIZE), 2)\n",
    "        \n",
    "        # Récupérer l'état actuel\n",
    "        board = obs[\"board\"]\n",
    "        valid_moves_array = obs[\"valid_moves\"]\n",
    "        \n",
    "        # Convertir le tableau valide_moves en liste de tuples\n",
    "        valid_moves = []\n",
    "        for i in range(len(valid_moves_array)):\n",
    "            if valid_moves_array[i] == 1:\n",
    "                row, col = i // BOARD_SIZE, i % BOARD_SIZE\n",
    "                valid_moves.append((row, col))\n",
    "        \n",
    "        # Dessiner les mouvements valides en surbrillance\n",
    "        for row, col in valid_moves:\n",
    "            screen.blit(valid_move_surface, (col * SQUARE_SIZE, row * SQUARE_SIZE))\n",
    "        \n",
    "        # Dessiner les pièces\n",
    "        for row in range(BOARD_SIZE):\n",
    "            for col in range(BOARD_SIZE):\n",
    "                center_x = col * SQUARE_SIZE + SQUARE_SIZE // 2\n",
    "                center_y = row * SQUARE_SIZE + SQUARE_SIZE // 2\n",
    "                \n",
    "                if board[row][col] == BLACK:\n",
    "                    pygame.draw.circle(screen, BLACK_COLOR, (center_x, center_y), \n",
    "                                      SQUARE_SIZE // 2 - 5)\n",
    "                elif board[row][col] == WHITE:\n",
    "                    pygame.draw.circle(screen, WHITE_COLOR, (center_x, center_y), \n",
    "                                      SQUARE_SIZE // 2 - 5)\n",
    "    \n",
    "    def draw_info_panel():\n",
    "        \"\"\"Dessine le panneau d'informations.\"\"\"\n",
    "        # Fond du panneau\n",
    "        pygame.draw.rect(screen, INFO_PANEL_COLOR, \n",
    "                         (BOARD_WIDTH, 0, INFO_PANEL_WIDTH, WINDOW_HEIGHT))\n",
    "        \n",
    "        # Titre\n",
    "        title = title_font.render(\"OTHELLO\", True, TEXT_COLOR)\n",
    "        screen.blit(title, (BOARD_WIDTH + INFO_PANEL_WIDTH // 2 - title.get_width() // 2, 20))\n",
    "        \n",
    "        # Agents\n",
    "        agents_title = info_font.render(f\"{agent1.name} (Noir) vs {agent2.name} (Blanc)\", True, TEXT_COLOR)\n",
    "        screen.blit(agents_title, (BOARD_WIDTH + INFO_PANEL_WIDTH // 2 - agents_title.get_width() // 2, 60))\n",
    "        \n",
    "        # Score\n",
    "        black_count, white_count = env._get_score()\n",
    "        \n",
    "        score_title = info_font.render(\"SCORE\", True, TEXT_COLOR)\n",
    "        screen.blit(score_title, (BOARD_WIDTH + INFO_PANEL_WIDTH // 2 - score_title.get_width() // 2, 100))\n",
    "        \n",
    "        score_black = score_font.render(f\"Noir: {black_count}\", True, TEXT_COLOR)\n",
    "        screen.blit(score_black, (BOARD_WIDTH + 20, 130))\n",
    "        \n",
    "        score_white = score_font.render(f\"Blanc: {white_count}\", True, TEXT_COLOR)\n",
    "        screen.blit(score_white, (BOARD_WIDTH + 20, 160))\n",
    "        \n",
    "        # Tour du joueur\n",
    "        current_player = \"Noir\" if obs[\"current_player\"] == 0 else \"Blanc\"\n",
    "        current_agent = agent1.name if obs[\"current_player\"] == 0 else agent2.name\n",
    "        player_text = info_font.render(f\"Tour: {current_player} ({current_agent})\", True, TEXT_COLOR)\n",
    "        screen.blit(player_text, (BOARD_WIDTH + 20, 200))\n",
    "        \n",
    "        # Récompenses cumulatives\n",
    "        reward_title = info_font.render(\"RÉCOMPENSES\", True, TEXT_COLOR)\n",
    "        screen.blit(reward_title, (BOARD_WIDTH + INFO_PANEL_WIDTH // 2 - reward_title.get_width() // 2, 240))\n",
    "        \n",
    "        reward_black = info_font.render(f\"Noir: {black_reward:.1f}\", True, TEXT_COLOR)\n",
    "        screen.blit(reward_black, (BOARD_WIDTH + 20, 270))\n",
    "        \n",
    "        reward_white = info_font.render(f\"Blanc: {white_reward:.1f}\", True, TEXT_COLOR)\n",
    "        screen.blit(reward_white, (BOARD_WIDTH + 20, 300))\n",
    "        \n",
    "        # Historique des actions\n",
    "        history_title = info_font.render(\"DERNIÈRES ACTIONS\", True, TEXT_COLOR)\n",
    "        screen.blit(history_title, (BOARD_WIDTH + INFO_PANEL_WIDTH // 2 - history_title.get_width() // 2, 340))\n",
    "        \n",
    "        # Afficher les 5 dernières actions\n",
    "        for i, (action, player) in enumerate(actions_history[-5:]):\n",
    "            row, col = action // BOARD_SIZE, action % BOARD_SIZE\n",
    "            player_name = \"Noir\" if player == 0 else \"Blanc\"\n",
    "            action_text = info_font.render(f\"{player_name}: ({row}, {col})\", True, TEXT_COLOR)\n",
    "            screen.blit(action_text, (BOARD_WIDTH + 20, 370 + i * 25))\n",
    "        \n",
    "        # Instructions\n",
    "        instructions = info_font.render(\"Appuyez sur ESPACE pour faire avancer\", True, TEXT_COLOR)\n",
    "        screen.blit(instructions, (BOARD_WIDTH + INFO_PANEL_WIDTH // 2 - instructions.get_width() // 2, 490))\n",
    "        \n",
    "        quit_instr = info_font.render(\"ou Q pour quitter\", True, TEXT_COLOR)\n",
    "        screen.blit(quit_instr, (BOARD_WIDTH + INFO_PANEL_WIDTH // 2 - quit_instr.get_width() // 2, 520))\n",
    "    \n",
    "    # Boucle principale de visualisation\n",
    "    running = True\n",
    "    auto_play = False\n",
    "    last_action_time = 0\n",
    "    \n",
    "    while running:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Gérer les événements\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            \n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_q:  # Quitter\n",
    "                    running = False\n",
    "                elif event.key == pygame.K_SPACE:  # Faire avancer manuellement\n",
    "                    if not done:\n",
    "                        auto_play = False\n",
    "                        last_action_time = 0  # Forcer la prochaine action\n",
    "                elif event.key == pygame.K_a:  # Mode automatique\n",
    "                    auto_play = not auto_play\n",
    "        \n",
    "        # Mode automatique ou action manuelle\n",
    "        if (auto_play and current_time - last_action_time > delay) or (not auto_play and last_action_time == 0):\n",
    "            if not done:\n",
    "                # Déterminer quel agent joue\n",
    "                current_player = obs[\"current_player\"]\n",
    "                current_agent = agent1 if current_player == 0 else agent2\n",
    "                \n",
    "                # Choisir une action\n",
    "                action = current_agent.choose_action(env)\n",
    "                \n",
    "                # Exécuter l'action\n",
    "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "                \n",
    "                # Enregistrer l'action dans l'historique\n",
    "                actions_history.append((action, current_player))\n",
    "                \n",
    "                # Mettre à jour les récompenses\n",
    "                if current_player == 0:  # BLACK\n",
    "                    black_reward += reward\n",
    "                else:  # WHITE\n",
    "                    white_reward += reward\n",
    "                \n",
    "                # Mettre à jour l'état\n",
    "                obs = next_obs\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Mettre à jour le temps de la dernière action\n",
    "                last_action_time = current_time\n",
    "            else:\n",
    "                # Partie terminée, afficher le résultat\n",
    "                black_count, white_count = env._get_score()\n",
    "                if black_count > white_count:\n",
    "                    print(f\"Noir ({agent1.name}) a gagné! {black_count}-{white_count}\")\n",
    "                elif white_count > black_count:\n",
    "                    print(f\"Blanc ({agent2.name}) a gagné! {white_count}-{black_count}\")\n",
    "                else:\n",
    "                    print(f\"Match nul! {black_count}-{white_count}\")\n",
    "                \n",
    "                # Attendre un peu avant de fermer\n",
    "                if auto_play:\n",
    "                    time.sleep(3)\n",
    "                    running = False\n",
    "        \n",
    "        # Dessiner le jeu\n",
    "        draw_board()\n",
    "        draw_info_panel()\n",
    "        \n",
    "        # Mettre à jour l'affichage\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        # Limiter la fréquence d'images\n",
    "        clock.tick(60)\n",
    "    \n",
    "    # Fermer pygame\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Supposons que train_loop() a renvoyé ces listes :\n",
    "# training_losses et training_rewards\n",
    "\n",
    "# Graphique de l'évolution des pertes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(training_losses, marker='o', label='Perte par itération')\n",
    "plt.plot(pd.Series(training_losses).rolling(3).mean(), label='Moyenne mobile (3 itérations)', color='red')\n",
    "plt.xlabel(\"Itération\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.title(\"Évolution de la perte pendant l'entraînement\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graphique de l'évolution des récompenses finales\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(training_rewards, marker='o', label='Récompense finale par partie')\n",
    "plt.plot(pd.Series(training_rewards).rolling(3).mean(), label='Moyenne mobile (3 itérations)', color='red')\n",
    "plt.xlabel(\"Itération\")\n",
    "plt.ylabel(\"Récompense finale\")\n",
    "plt.title(\"Évolution des récompenses pendant l'entraînement\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(self, env):\n",
    "    obs = env._get_observation()\n",
    "    board = obs[\"board\"]\n",
    "    state_input = torch.tensor(board, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "    with torch.no_grad():\n",
    "        network_output = self.network(state_input)\n",
    "    policy_logits = network_output.policy_logits\n",
    "    probs = torch.softmax(policy_logits, dim=1)\n",
    "    action = torch.argmax(probs, dim=1).item()\n",
    "    return action\n",
    "\n",
    "random_agent = RandomAgent()\n",
    "\n",
    "# Load your trained model (if you haven't already)\n",
    "network = SimpleAlphaZeroNet()\n",
    "network.load_state_dict(torch.load(\"alpha_zero_model1.pt\", map_location=torch.device(\"cpu\")))\n",
    "network.eval()\n",
    "\n",
    "agent.name= \"AlphaGo\"\n",
    "random_agent.name = \"Random\"\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_agent(agent, random_agent, num_games=10, render=False)\n",
    "print(\"Résultats:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE=8\n",
    "# Visualisation d'une partie entre l'agent AlphaGo et l'agent aléatoire\n",
    "print(\"Visualisation d'une partie entre l'agent AlphaGo et l'agent aléatoire...\")\n",
    "print(\"Utilisez ESPACE pour faire avancer manuellement, A pour activer le mode automatique, Q pour quitter.\")\n",
    "visualize_game(agent, random_agent, delay=1.0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
